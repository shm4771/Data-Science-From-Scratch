{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv:\n",
    "    def __init__(self, input_shape, filter_shape, num_filters=8):\n",
    "        \"\"\"\n",
    "        input_shape -> channels * height * width\n",
    "        filter_shape -> filter height * filter width\n",
    "        num_filters -> number of filters\n",
    "        output_shape -> num_filters * height_out * width_out\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        Notations\n",
    "        c ->  channels\n",
    "        h -> image height\n",
    "        w -> image width\n",
    "        f -> number of filters\n",
    "        fh -> filter height\n",
    "        fw -> filter width\n",
    "        h_out -> output image height\n",
    "        w_out -> output image width\n",
    "        \"\"\"\n",
    "        \n",
    "        self.input_shape = (1,) + input_shape #input shape (1, c, h, w)\n",
    "        self.filter_shape = (num_filters, self.input_shape[1], *filter_shape)#filter shape (f, c, fh,fw)\n",
    "        \n",
    "        # filter initialization (f * c * fh * fw)\n",
    "        c, fh, fw = self.filter_shape[1:]\n",
    "        self.filters = np.random.randn(*self.filter_shape)/(fh * fw * c) # dividing by (fh*fw) is important for xavier initialization\n",
    "        self.b = np.zeros(self.filter_shape[0])  #bias term initialization\n",
    "    \n",
    "    def get_output_shape(self):\n",
    "        \"\"\" returns the shape of the output of convolution operation\"\"\"\n",
    "        h_out = self.input_shape[2] + 1 - self.filter_shape[2] #stride has been taken as 1\n",
    "        w_out = self.input_shape[3] + 1 - self.filter_shape[3] #stride has been taken as 1\n",
    "        return (self.filter_shape[0], h_out, w_out) #output shape (f, h_out, w_out)\n",
    "        \n",
    "        \n",
    "    def iter_input(self, input_data, fh, fw, h_iter, w_iter):\n",
    "        \"\"\" returns the slices of input data.\n",
    "            fh, fw -> filter heights, filter widths\n",
    "            h_iter, w_iter -> number of steps \"\"\"\n",
    "        for i in range(h_iter):\n",
    "            for j in range(w_iter):\n",
    "                img = input_data[:, :, i:i+fh, j:j+fw]\n",
    "                yield i, j, img\n",
    "        \n",
    "    def forward(self, input_data, padding=False):\n",
    "        \"\"\" computes output of layer on given input of shape (c, h, w) \"\"\"\n",
    "        input_data = input_data[np.newaxis, ...] #convert input data in shape (1, c, h, w)\n",
    "        self.last_input = input_data #input saved for backprop phase\n",
    "        \n",
    "        h_out, w_out = self.get_output_shape()[1:] \n",
    "        fh, fw = self.filter_shape[2:]\n",
    "        output = np.zeros(self.get_output_shape()) # (f, h_out, w_out) \n",
    "        \n",
    "        for i, j, img_area in self.iter_input(input_data, fh, fw, h_out, w_out):\n",
    "            output[:,i,j] = np.sum(self.filters * img_area, axis=(1, 2, 3)) + self.b #we are collapsing h,w dimensions by finding sum\n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def backward(self, gradients, learn_rate=0.005):\n",
    "        \"\"\" accept gradients as input in shape of (f, h_out, w_out) and \n",
    "        returns gradients as output in shape of (c, h, w) \"\"\"\n",
    "        #print(\"max grad\", np.max(gradients), np.min(gradients))\n",
    "        gradients = gradients.reshape((self.filter_shape[0], 1, gradients.shape[1], gradients.shape[2]))#convert gradients in shape (f, 1, h_out, w_out)\n",
    "        \n",
    "        #gradient terms intialization\n",
    "        grad_filters = np.zeros(self.filters.shape)\n",
    "        grad_b = np.zeros_like(self.b)\n",
    "        grad_x = np.zeros(self.input_shape) #gradienst w. r. t. inputs\n",
    "        \n",
    "        grad_b = np.sum(gradients, axis=(1,2,3)) #easy huh? :)\n",
    "        \n",
    "        fh, fw = gradients.shape[2:] #filter sizes\n",
    "        h_steps, w_steps = self.filter_shape[2:] #filter steps in both direction \n",
    "        for i, j, img_area in self.iter_input(self.last_input, fh, fw, h_steps, w_steps):\n",
    "            grad_filters[:, :, i, j] = np.sum(img_area * gradients, axis=(2, 3)) \n",
    "        \n",
    "        #update the gradients of the filters \n",
    "        #print(\"grad filters\", grad_filters)\n",
    "        #print(\"grad b\", grad_b)\n",
    "        self.grad_filters = grad_filters\n",
    "        self.grad_b = grad_b\n",
    "        self.filters -= learn_rate * grad_filters\n",
    "        self.b -= learn_rate * grad_b\n",
    "        #print(\"grad filter max\", np.max(grad_filters))\n",
    "        #what about grad_x :(\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1 0]\n",
      "  [0 1]]]\n",
      "[[0.58758998 0.        ]\n",
      " [0.         0.58758998]]\n",
      "[[0.58758998]]\n",
      "[0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "conv = Conv((1, 2, 2), (1, 1), 3)\n",
    "img = np.array([[[1, 0], [0, 1]]])\n",
    "out = conv.forward(img)\n",
    "print(img)\n",
    "print(out[0])\n",
    "print(conv.filters[0, 0])\n",
    "print(conv.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maxpool:\n",
    "    #this will apply max function to the input obtained from Conv layer\n",
    "    def __init__(self, input_shape, pool_size=2):\n",
    "        self.input_shape = input_shape\n",
    "        self.pool_size = pool_size\n",
    "        \n",
    "    def get_output_shape(self):\n",
    "        self.output_shape = list(self.input_shape)\n",
    "        self.output_shape[0] = self.input_shape[0] // self.pool_size\n",
    "        self.output_shape[1] = self.input_shape[1] // self.pool_size\n",
    "        return tuple(self.output_shape)\n",
    "    \n",
    "    def iter_input(self, input_data):\n",
    "        output_shape = self.get_output_shape()\n",
    "        out_h, out_w = output_shape[0], output_shape[1]\n",
    "        for i in range(out_h):\n",
    "            for j in range(out_w):\n",
    "                yield input_data[self.pool_size*i:self.pool_size*(i+1), self.pool_size*j:self.pool_size*(j+1)], i, j\n",
    "    \n",
    "    def forward(self, input_data):\n",
    "        \"\"\" This function computes output after applying max filter\"\"\"\n",
    "        self.last_input = input_data #saving data for backprop\n",
    "        output = np.zeros(self.get_output_shape())\n",
    "        for img_area, i, j in self.iter_input(input_data):\n",
    "            out = np.amax(img_area, axis=(0, 1))\n",
    "            output[i, j] = out\n",
    "        return output\n",
    "        \n",
    "    def backward(self, gradients, learn_rate=0.03):\n",
    "        \"\"\" take gradients of shape (w,h,t,f1, f2, ...) as input and \n",
    "        returns gradients as output in the same shape\"\"\"\n",
    "        \n",
    "        h, w = self.last_input.shape[0], self.last_input.shape[0]\n",
    "        h_out, w_out = gradients.shape[0], gradients.shape[1]\n",
    "    \n",
    "        total_entries = np.array(self.last_input.shape).prod()\n",
    "        changed_last_inputs = self.last_input.reshape((w,h, int(total_entries/(w*h))))\n",
    "        out_grad = np.zeros(changed_last_inputs.shape)\n",
    "        changed_gradients = gradients.reshape((w_out, h_out, changed_last_inputs.shape[2]))\n",
    "        \n",
    "        for img_area, i, j in self.iter_input(changed_last_inputs):\n",
    "            max_val = np.amax(img_area, axis=(0, 1))\n",
    "            for k, val in enumerate(max_val):\n",
    "                x, y = np.where(img_area[:, :, k]==val)\n",
    "                x, y = int(x[0]), int(y[0])\n",
    "                changed_output_grad[self.pool_size * i + x, self.pool_size*j + y, k] = changed_gradients[i, j, k]     \n",
    "        out_grad = changed_output_grad.reshape(self.input_shape)\n",
    "        return out_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3, 8)\n"
     ]
    }
   ],
   "source": [
    "max_layer = Maxpool((7,7, 8),2)\n",
    "img = np.ones((7, 7, 8))\n",
    "out = max_layer.get_output_shape()\n",
    "#print(out)\n",
    "output = max_layer.forward(img)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing both the layers \n",
    "#forward phase\n",
    "layer_1 = Conv((28, 28), num_filters=8)\n",
    "#print(layer_1.get_output_shape())\n",
    "img = np.ones((28, 28))\n",
    "output = layer_1.forward(img)\n",
    "max_layer = Maxpool(output.shape,2)\n",
    "out = max_layer.get_output_shape()\n",
    "#print(out)\n",
    "output = max_layer.forward(img)\n",
    "print(output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 4, 3, 2)\n",
      "(9, 9, 3, 2)\n"
     ]
    }
   ],
   "source": [
    "#checking backprop of the  Maxpool layer\n",
    "max_layer = Maxpool((9, 9, 3, 2), 2)\n",
    "img = np.random.randint(250, size=(9,9,3,2))\n",
    "out_1 = max_layer.forward(img)\n",
    "grad = np.random.randint(10, size=out_1.shape)\n",
    "accha = max_layer.backward(grad)\n",
    "print(grad.shape)\n",
    "print(accha.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self, input_len, nodes=100):\n",
    "        self.input_len = input_len\n",
    "        self.output_len = nodes\n",
    "        \n",
    "        #initializing weight and biases\n",
    "        self.W = np.random.randn(self.input_len, self.output_len) / (self.input_len * self.output_len) #shape(output_len, input_len)\n",
    "        self.b = np.zeros(self.output_len)#shape(output_len, )\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        self.original_input_shape = input_data.shape #for backward phase\n",
    "        \n",
    "        #first we will flatten the input data\n",
    "        input_data = input_data.flatten() #shape (input_len, )\n",
    "        input_data = input_data / np.max(input_data)\n",
    "        #input_data = input_data[:, np.newaxis] #shape (input_len, 1)\n",
    "        z = np.dot(input_data, self.W) + self.b #shape (output_len, )\n",
    "        self.last_input = input_data #flattened input for backward phase\n",
    "        #z = z - np.max(z)\n",
    "        self.last_z = z #z vector for backward phase\n",
    "        #if (abs(np.sum(np.exp(z), axis=0))<=10**-8):\n",
    "        #    print(\"z went 0\", z)\n",
    "        return np.exp(z) / np.sum(np.exp(z), axis=0) \n",
    "    \n",
    "    def backward(self, gradients, learn_rate=0.005):\n",
    "        \"\"\" it takes gradients with respect to layer-outputs (dcost/da^(L)) as input, update weights and biases, and\n",
    "        return gradients with respect to the layer-inputs(dcost/da^(L-1))\"\"\"\n",
    "        \n",
    "\n",
    "        #finding the index where gradient is non zero\n",
    "        index = int(np.where(gradients != 0)[0])\n",
    "        \n",
    "        #denominator term of softmax function\n",
    "        S = np.sum(np.exp(self.last_z)) \n",
    "        \n",
    "        #dp_dz vector\n",
    "        dp_dz = np.zeros_like(gradients) #(output_len, )\n",
    "        dp_dz = - (np.exp(self.last_z)[index] * np.exp(self.last_z))/ (S ** 2)\n",
    "        dp_dz[index] = np.exp(self.last_z)[index] *(S - np.exp(self.last_z)[index])/ (S ** 2) \n",
    "\n",
    "        \n",
    "        #dc_dz\n",
    "        dc_dz = gradients[index] * dp_dz\n",
    "        #gradients of cost w.r.t weights, biases and inputs\n",
    "        \n",
    "        \n",
    "        dc_Dw = self.last_input[np.newaxis].T @ dc_dz[np.newaxis] \n",
    "        dc_db = dc_dz\n",
    "        dc_da = self.W @ dc_dz\n",
    "        #update the weights and biases\n",
    "        self.W -= learn_rate * dc_Dw\n",
    "        self.b -= learn_rate * dc_db\n",
    "        \n",
    "        #reshape the input gradients in original input form and return\n",
    "        return dc_da.reshape(self.original_input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vect = np.array([[0,1], [1,0]])\n",
    "soft = Softmax(4, 2)\n",
    "out = soft.forward(input_vect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25906534 0.74093466]\n",
      "[3.7660374e-05 9.9996234e-01]\n"
     ]
    }
   ],
   "source": [
    "#test the softmax\n",
    "input_vect = np.array([[2,2], [1,3]])\n",
    "soft = Softmax(4, 2)\n",
    "out = soft.forward(input_vect)\n",
    "print(out)\n",
    "y = 1\n",
    "for i in range(100):\n",
    "    grad = np.zeros(out.shape)\n",
    "    grad[y] = -1/np.log(out[y])\n",
    "    soft.backward(grad)\n",
    "    out = soft.forward(input_vect)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 28, 28)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "##loading the data \n",
    "data = mnist.load_data()\n",
    "((x_train, y_train), (x_test, y_test)) = data\n",
    "x_train, y_train = x_train[:1000], y_train[:1000]\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "x = x_train[0][np.newaxis,...]\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(img, label):\n",
    "    #forward step\n",
    "    out = conv.forward((img[np.newaxis, ...]/255)-0.5)\n",
    "    #out = conv_2.forward(out)\n",
    "    #out = pool.forward(out)\n",
    "    out = soft.forward(out)\n",
    "    #compute loss and accuracy\n",
    "    loss = -max(np.log(out[label]), -10**10) #cross entropy\n",
    "    acc = 1 if (np.argmax(out) == label) else 0\n",
    "    \n",
    "    return out, loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(img, label, lr=0.005):\n",
    "    #forward\n",
    "    out, loss, acc = forward(img, label)\n",
    "    \n",
    "    #calculate gradients\n",
    "    gradient = np.zeros(10)\n",
    "    gradient[label] = -1/max(out[label], 10 ** -6)\n",
    "    #print(\"gradient\", gradient)\n",
    "    #backprop\n",
    "    gradient = soft.backward(gradient, lr)\n",
    "    #gradient = pool.backward(gradient)\n",
    "    gradient = conv.backward(gradient)\n",
    "    #print(\"z\", soft.last_z.flatten(), label)\n",
    "    #print(\"p\", out)\n",
    "    #print (\"dc_dz\", soft.dc_dz.flatten())\n",
    "\n",
    "    \n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250\n",
      "CNN initialized\n",
      "running the epoch; 0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'conv_2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-252-1fb1c622569c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mnum_correct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mnum_correct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-251-fc2a0c24962f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(img, label, lr)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.005\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m#forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#calculate gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-250-40c92325870d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(img, label)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m#forward step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;31m#out = pool.forward(out)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'conv_2' is not defined"
     ]
    }
   ],
   "source": [
    "input_shape = (1, 28, 28)\n",
    "conv  = Conv(input_shape, filter_shape=(4,4), num_filters=2)\n",
    "conv_shape = conv.get_output_shape()\n",
    "#conv  = Conv(input_shape, filter_shape=(4,4), num_filters=2)\n",
    "#conv_shape = conv.get_output_shape()\n",
    "#pool = Maxpool(conv_shape, 2)\n",
    "#pool_shape = pool.get_output_shape()\n",
    "num_features = np.prod(np.array(conv_shape))\n",
    "print(num_features)\n",
    "output_features = 10\n",
    "soft = Softmax(num_features, output_features)\n",
    "\n",
    "print('CNN initialized')\n",
    "for epoch in range(2):\n",
    "    print(\"running the epoch;\", epoch)\n",
    "    #shuffle the training examples\n",
    "    permutation = np.random.permutation(len(x_train))\n",
    "    x_train = x_train[permutation]\n",
    "    y_train = y_train[permutation]\n",
    "\n",
    "    #Training the forward phase\n",
    "    num_correct = 0\n",
    "    loss = 0\n",
    "    for i, (img, label) in enumerate(zip(x_train, y_train)):\n",
    "        #print(\"count\", i)\n",
    "        if i % 100 == 99:\n",
    "            print(\n",
    "                '[Step %d] Past 100 steps: Average Loss %.3f | Accuracy: %d%%' %\n",
    "                (i + 1, loss / 100, num_correct)\n",
    "            )\n",
    "            #print(\"z\", soft.last_z.flatten(), label)\n",
    "            loss = 0\n",
    "            num_correct = 0\n",
    "\n",
    "        l, acc = train(img, label)\n",
    "        loss += l\n",
    "        num_correct += acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6760\n",
      "CNN initialized\n",
      "running the epoch; 0\n",
      "[Step 100] Past 100 steps: Average Loss 1.614 | Accuracy: 48%\n",
      "[Step 200] Past 100 steps: Average Loss 1.116 | Accuracy: 59%\n",
      "[Step 300] Past 100 steps: Average Loss 0.878 | Accuracy: 71%\n",
      "[Step 400] Past 100 steps: Average Loss 0.977 | Accuracy: 68%\n",
      "[Step 500] Past 100 steps: Average Loss 0.726 | Accuracy: 77%\n",
      "[Step 600] Past 100 steps: Average Loss 0.993 | Accuracy: 67%\n",
      "[Step 700] Past 100 steps: Average Loss 0.742 | Accuracy: 76%\n",
      "[Step 800] Past 100 steps: Average Loss 0.868 | Accuracy: 69%\n",
      "[Step 900] Past 100 steps: Average Loss 0.743 | Accuracy: 75%\n",
      "[Step 1000] Past 100 steps: Average Loss 0.685 | Accuracy: 75%\n",
      "[Step 1100] Past 100 steps: Average Loss 0.656 | Accuracy: 78%\n",
      "[Step 1200] Past 100 steps: Average Loss 0.663 | Accuracy: 77%\n",
      "[Step 1300] Past 100 steps: Average Loss 0.563 | Accuracy: 80%\n",
      "[Step 1400] Past 100 steps: Average Loss 0.513 | Accuracy: 81%\n",
      "[Step 1500] Past 100 steps: Average Loss 0.908 | Accuracy: 72%\n",
      "[Step 1600] Past 100 steps: Average Loss 0.509 | Accuracy: 80%\n",
      "[Step 1700] Past 100 steps: Average Loss 0.648 | Accuracy: 80%\n",
      "[Step 1800] Past 100 steps: Average Loss 0.730 | Accuracy: 78%\n",
      "[Step 1900] Past 100 steps: Average Loss 0.677 | Accuracy: 75%\n",
      "[Step 2000] Past 100 steps: Average Loss 0.613 | Accuracy: 74%\n",
      "running the epoch; 1\n",
      "[Step 100] Past 100 steps: Average Loss 0.425 | Accuracy: 86%\n",
      "[Step 200] Past 100 steps: Average Loss 0.408 | Accuracy: 85%\n",
      "[Step 300] Past 100 steps: Average Loss 0.715 | Accuracy: 78%\n",
      "[Step 400] Past 100 steps: Average Loss 0.401 | Accuracy: 88%\n",
      "[Step 500] Past 100 steps: Average Loss 0.497 | Accuracy: 86%\n",
      "[Step 600] Past 100 steps: Average Loss 0.532 | Accuracy: 83%\n",
      "[Step 700] Past 100 steps: Average Loss 0.332 | Accuracy: 88%\n",
      "[Step 800] Past 100 steps: Average Loss 0.462 | Accuracy: 82%\n",
      "[Step 900] Past 100 steps: Average Loss 0.529 | Accuracy: 83%\n",
      "[Step 1000] Past 100 steps: Average Loss 0.305 | Accuracy: 92%\n",
      "[Step 1100] Past 100 steps: Average Loss 0.427 | Accuracy: 86%\n",
      "[Step 1200] Past 100 steps: Average Loss 0.537 | Accuracy: 80%\n",
      "[Step 1300] Past 100 steps: Average Loss 0.598 | Accuracy: 79%\n",
      "[Step 1400] Past 100 steps: Average Loss 0.576 | Accuracy: 81%\n",
      "[Step 1500] Past 100 steps: Average Loss 0.461 | Accuracy: 87%\n",
      "[Step 1600] Past 100 steps: Average Loss 0.501 | Accuracy: 83%\n",
      "[Step 1700] Past 100 steps: Average Loss 0.367 | Accuracy: 91%\n",
      "[Step 1800] Past 100 steps: Average Loss 0.405 | Accuracy: 87%\n"
     ]
    }
   ],
   "source": [
    "input_shape = (1, 28, 28)\n",
    "conv  = Conv(input_shape, filter_shape=(3,3), num_filters=10)\n",
    "conv_shape = conv.get_output_shape()\n",
    "#conv_2 = Conv(conv_shape, filter_shape=(3,3), num_filters=3)\n",
    "#conv_2_shape = conv_2.get_output_shape()\n",
    "#conv  = Conv(input_shape, filter_shape=(4,4), num_filters=2)\n",
    "#conv_shape = conv.get_output_shape()\n",
    "#pool = Maxpool(conv_shape, 2)\n",
    "#pool_shape = pool.get_output_shape()\n",
    "num_features = np.prod(np.array(conv_shape))\n",
    "print(num_features)\n",
    "output_features = 10\n",
    "soft = Softmax(num_features, output_features)\n",
    "\n",
    "print('CNN initialized')\n",
    "for epoch in range(3):\n",
    "    print(\"running the epoch;\", epoch)\n",
    "    #shuffle the training examples\n",
    "    permutation = np.random.permutation(len(x_train))\n",
    "    x_train = x_train[permutation]\n",
    "    y_train = y_train[permutation]\n",
    "\n",
    "    #Training the forward phase\n",
    "    num_correct = 0\n",
    "    loss = 0\n",
    "    for i, (img, label) in enumerate(zip(x_train, y_train)):\n",
    "        #print(\"count\", i)\n",
    "        if i % 100 == 99:\n",
    "            print(\n",
    "                '[Step %d] Past 100 steps: Average Loss %.3f | Accuracy: %d%%' %\n",
    "                (i + 1, loss / 100, num_correct)\n",
    "            )\n",
    "            #print(\"z\", soft.last_z.flatten(), label)\n",
    "            loss = 0\n",
    "            num_correct = 0\n",
    "\n",
    "        l, acc = train(img, label)\n",
    "        loss += l\n",
    "        num_correct += acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdd7f2c24a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#visualizing the filters\n",
    "img = x_train[3]\n",
    "plt.imshow(img, cmap='gray', vmin=0, vmax=255)\n",
    "img = img[np.newaxis, ...]\n",
    "out = conv.forward(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fdd7f06fe48>"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdd7f0b2588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(out[4], cmap='gray', vmin=0, vmax=255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.13901809,  0.12804853,  0.05074609],\n",
       "       [ 0.00905267, -0.0960619 ,  0.18055336],\n",
       "       [ 0.09185021,  0.19787026,  0.00338203]])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.filters[2, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "x_train, y_train = x_train[:2000], y_train[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4608\n",
      "CNN initialized\n",
      "running the epoch; 0\n",
      "[Step 100] Past 100 steps: Average Loss 1.552 | Accuracy: 46%\n",
      "[Step 200] Past 100 steps: Average Loss 1.117 | Accuracy: 60%\n",
      "[Step 300] Past 100 steps: Average Loss 0.899 | Accuracy: 66%\n",
      "[Step 400] Past 100 steps: Average Loss 0.798 | Accuracy: 73%\n",
      "[Step 500] Past 100 steps: Average Loss 0.995 | Accuracy: 63%\n",
      "[Step 600] Past 100 steps: Average Loss 0.891 | Accuracy: 67%\n",
      "[Step 700] Past 100 steps: Average Loss 0.874 | Accuracy: 72%\n",
      "[Step 800] Past 100 steps: Average Loss 0.776 | Accuracy: 72%\n",
      "[Step 900] Past 100 steps: Average Loss 0.757 | Accuracy: 76%\n",
      "[Step 1000] Past 100 steps: Average Loss 0.774 | Accuracy: 72%\n",
      "[Step 1100] Past 100 steps: Average Loss 0.705 | Accuracy: 76%\n",
      "[Step 1200] Past 100 steps: Average Loss 0.825 | Accuracy: 78%\n",
      "[Step 1300] Past 100 steps: Average Loss 0.796 | Accuracy: 73%\n",
      "[Step 1400] Past 100 steps: Average Loss 0.680 | Accuracy: 79%\n",
      "[Step 1500] Past 100 steps: Average Loss 0.578 | Accuracy: 79%\n",
      "[Step 1600] Past 100 steps: Average Loss 0.511 | Accuracy: 82%\n",
      "[Step 1700] Past 100 steps: Average Loss 0.532 | Accuracy: 80%\n",
      "[Step 1800] Past 100 steps: Average Loss 0.537 | Accuracy: 75%\n",
      "[Step 1900] Past 100 steps: Average Loss 0.595 | Accuracy: 78%\n",
      "[Step 2000] Past 100 steps: Average Loss 0.712 | Accuracy: 74%\n",
      "running the epoch; 1\n",
      "[Step 100] Past 100 steps: Average Loss 0.524 | Accuracy: 82%\n",
      "[Step 200] Past 100 steps: Average Loss 0.426 | Accuracy: 85%\n",
      "[Step 300] Past 100 steps: Average Loss 0.459 | Accuracy: 85%\n",
      "[Step 400] Past 100 steps: Average Loss 0.540 | Accuracy: 80%\n",
      "[Step 500] Past 100 steps: Average Loss 0.427 | Accuracy: 87%\n",
      "[Step 600] Past 100 steps: Average Loss 0.629 | Accuracy: 80%\n",
      "[Step 700] Past 100 steps: Average Loss 0.518 | Accuracy: 81%\n",
      "[Step 800] Past 100 steps: Average Loss 0.439 | Accuracy: 88%\n",
      "[Step 900] Past 100 steps: Average Loss 0.675 | Accuracy: 77%\n",
      "[Step 1000] Past 100 steps: Average Loss 0.596 | Accuracy: 79%\n",
      "[Step 1100] Past 100 steps: Average Loss 0.562 | Accuracy: 80%\n",
      "[Step 1200] Past 100 steps: Average Loss 0.573 | Accuracy: 83%\n",
      "[Step 1300] Past 100 steps: Average Loss 0.529 | Accuracy: 84%\n",
      "[Step 1400] Past 100 steps: Average Loss 0.650 | Accuracy: 78%\n",
      "[Step 1500] Past 100 steps: Average Loss 0.610 | Accuracy: 79%\n",
      "[Step 1600] Past 100 steps: Average Loss 0.524 | Accuracy: 83%\n",
      "[Step 1700] Past 100 steps: Average Loss 0.710 | Accuracy: 75%\n",
      "[Step 1800] Past 100 steps: Average Loss 0.507 | Accuracy: 85%\n",
      "[Step 1900] Past 100 steps: Average Loss 0.532 | Accuracy: 82%\n",
      "[Step 2000] Past 100 steps: Average Loss 0.614 | Accuracy: 78%\n"
     ]
    }
   ],
   "source": [
    "input_shape = (1, 28, 28)\n",
    "conv  = Conv(input_shape, filter_shape=(5,5), num_filters=8)\n",
    "conv_shape = conv.get_output_shape()\n",
    "#conv  = Conv(input_shape, filter_shape=(4,4), num_filters=2)\n",
    "#conv_shape = conv.get_output_shape()\n",
    "#pool = Maxpool(conv_shape, 2)\n",
    "#pool_shape = pool.get_output_shape()\n",
    "num_features = np.prod(np.array(conv_shape))\n",
    "print(num_features)\n",
    "output_features = 10\n",
    "soft = Softmax(num_features, output_features)\n",
    "\n",
    "print('CNN initialized')\n",
    "for epoch in range(2):\n",
    "    print(\"running the epoch;\", epoch)\n",
    "    #shuffle the training examples\n",
    "    permutation = np.random.permutation(len(x_train))\n",
    "    x_train = x_train[permutation]\n",
    "    y_train = y_train[permutation]\n",
    "\n",
    "    #Training the forward phase\n",
    "    num_correct = 0\n",
    "    loss = 0\n",
    "    for i, (img, label) in enumerate(zip(x_train, y_train)):\n",
    "        #print(\"count\", i)\n",
    "        if i % 100 == 99:\n",
    "            print(\n",
    "                '[Step %d] Past 100 steps: Average Loss %.3f | Accuracy: %d%%' %\n",
    "                (i + 1, loss / 100, num_correct)\n",
    "            )\n",
    "            #print(\"z\", soft.last_z.flatten(), label)\n",
    "            loss = 0\n",
    "            num_correct = 0\n",
    "\n",
    "        l, acc = train(img, label)\n",
    "        loss += l\n",
    "        num_correct += acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
